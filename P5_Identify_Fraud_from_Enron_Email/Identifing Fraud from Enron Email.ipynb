{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "In 2000, Enron was one of the largest companies in the United States. By 2002, it had collapsed into bankruptcy due to widespread corporate fraud. In the resulting Federal investigation, a significant amount of typically confidential information entered into the public record, including tens of thousands of emails and detailed financial data for top executives. In this project, I'm putting my new skills by building a person of interest identifier based on financial and email data made public as a result of the Enron scandal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding the Dataset and Question\n",
    "\n",
    "The goal of the project is to identify employees from Enron who may have committed fraud based on the public Enron financial and email dataset, i.e., a person of interest. We define a person of interest (POI) as an individual who was indicted, reached a settlement or plea deal with the government, or testified in exchange for prosecution immunity.\n",
    "\n",
    "Machine learning algorithms are useful in trying to accomplish goals like this one because they can process datasets way faster than humans and they can spot relevant trends that humans would have a hard time realizing manually. Here is some background on the Enron financial and email dataset.\n",
    "\n",
    "* Employees\n",
    "\n",
    "There are 146 Enron employees in the dataset. 18 of them are POIs.\n",
    "\n",
    "* Features\n",
    "\n",
    "There are fourteen (14) financial features. All units are US dollars.\n",
    "\n",
    "* salary\n",
    "* deferral_payments\n",
    "* total_payments\n",
    "* loan_advances\n",
    "* bonus\n",
    "* restricted_stock_deferred\n",
    "* deferred_income\n",
    "* total_stock_value\n",
    "* expenses\n",
    "* exercised_stock_options\n",
    "* other\n",
    "* long_term_incentive\n",
    "* restricted_stock\n",
    "* director_fees\n",
    "\n",
    "There are six (6) email features. All units are number of emails messages, except for ‘email_address’, which is a text string.\n",
    "\n",
    "* to_messages\n",
    "* email_address\n",
    "* from_poi_to_this_person\n",
    "* from_messages\n",
    "* from_this_person_to_poi\n",
    "* shared_receipt_with_poi\n",
    "\n",
    "There is one (1) other feature, which is a boolean, indicating whether or not the employee is a person of interest.\n",
    "* poi\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def poiEmails():\n",
    "    email_list = [\"kenneth_lay@enron.net\",    \n",
    "            \"kenneth_lay@enron.com\",\n",
    "            \"klay.enron@enron.com\",\n",
    "            \"kenneth.lay@enron.com\", \n",
    "            \"klay@enron.com\",\n",
    "            \"layk@enron.com\",\n",
    "            \"chairman.ken@enron.com\",\n",
    "            \"jeffreyskilling@yahoo.com\",\n",
    "            \"jeff_skilling@enron.com\",\n",
    "            \"jskilling@enron.com\",\n",
    "            \"effrey.skilling@enron.com\",\n",
    "            \"skilling@enron.com\",\n",
    "            \"jeffrey.k.skilling@enron.com\",\n",
    "            \"jeff.skilling@enron.com\",\n",
    "            \"kevin_a_howard.enronxgate.enron@enron.net\",\n",
    "            \"kevin.howard@enron.com\",\n",
    "            \"kevin.howard@enron.net\",\n",
    "            \"kevin.howard@gcm.com\",\n",
    "            \"michael.krautz@enron.com\"\n",
    "            \"scott.yeager@enron.com\",\n",
    "            \"syeager@fyi-net.com\",\n",
    "            \"scott_yeager@enron.net\",\n",
    "            \"syeager@flash.net\",\n",
    "            \"joe'.'hirko@enron.com\", \n",
    "            \"joe.hirko@enron.com\", \n",
    "            \"rex.shelby@enron.com\", \n",
    "            \"rex.shelby@enron.nt\", \n",
    "            \"rex_shelby@enron.net\",\n",
    "            \"jbrown@enron.com\",\n",
    "            \"james.brown@enron.com\", \n",
    "            \"rick.causey@enron.com\", \n",
    "            \"richard.causey@enron.com\", \n",
    "            \"rcausey@enron.com\",\n",
    "            \"calger@enron.com\",\n",
    "            \"chris.calger@enron.com\", \n",
    "            \"christopher.calger@enron.com\", \n",
    "            \"ccalger@enron.com\",\n",
    "            \"tim_despain.enronxgate.enron@enron.net\", \n",
    "            \"tim.despain@enron.com\",\n",
    "            \"kevin_hannon@enron.com\", \n",
    "            \"kevin'.'hannon@enron.com\", \n",
    "            \"kevin_hannon@enron.net\", \n",
    "            \"kevin.hannon@enron.com\",\n",
    "            \"mkoenig@enron.com\", \n",
    "            \"mark.koenig@enron.com\",\n",
    "            \"m..forney@enron.com\",\n",
    "            \"ken'.'rice@enron.com\", \n",
    "            \"ken.rice@enron.com\",\n",
    "            \"ken_rice@enron.com\", \n",
    "            \"ken_rice@enron.net\",\n",
    "            \"paula.rieker@enron.com\",\n",
    "            \"prieker@enron.com\", \n",
    "            \"andrew.fastow@enron.com\", \n",
    "            \"lfastow@pdq.net\", \n",
    "            \"andrew.s.fastow@enron.com\", \n",
    "            \"lfastow@pop.pdq.net\", \n",
    "            \"andy.fastow@enron.com\",\n",
    "            \"david.w.delainey@enron.com\", \n",
    "            \"delainey.dave@enron.com\", \n",
    "            \"'delainey@enron.com\", \n",
    "            \"david.delainey@enron.com\", \n",
    "            \"'david.delainey'@enron.com\", \n",
    "            \"dave.delainey@enron.com\", \n",
    "            \"delainey'.'david@enron.com\",\n",
    "            \"ben.glisan@enron.com\", \n",
    "            \"bglisan@enron.com\", \n",
    "            \"ben_f_glisan@enron.com\", \n",
    "            \"ben'.'glisan@enron.com\",\n",
    "            \"jeff.richter@enron.com\", \n",
    "            \"jrichter@nwlink.com\",\n",
    "            \"lawrencelawyer@aol.com\", \n",
    "            \"lawyer'.'larry@enron.com\", \n",
    "            \"larry_lawyer@enron.com\", \n",
    "            \"llawyer@enron.com\", \n",
    "            \"larry.lawyer@enron.com\", \n",
    "            \"lawrence.lawyer@enron.com\",\n",
    "            \"tbelden@enron.com\", \n",
    "            \"tim.belden@enron.com\", \n",
    "            \"tim_belden@pgn.com\", \n",
    "            \"tbelden@ect.enron.com\",\n",
    "            \"michael.kopper@enron.com\",\n",
    "            \"dave.duncan@enron.com\", \n",
    "            \"dave.duncan@cipco.org\", \n",
    "            \"duncan.dave@enron.com\",\n",
    "            \"ray.bowen@enron.com\", \n",
    "            \"raymond.bowen@enron.com\", \n",
    "            \"'bowen@enron.com\",\n",
    "            \"wes.colwell@enron.com\",\n",
    "            \"dan.boyle@enron.com\",\n",
    "            \"cloehr@enron.com\", \n",
    "            \"chris.loehr@enron.com\"\n",
    "        ]\n",
    "    return email_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=75, splitter='best')\n",
      "\tAccuracy: 0.90880\tPrecision: 0.66255\tRecall: 0.64400\tF1: 0.65314\tF2: 0.64763\n",
      "\tTotal predictions: 15000\tTrue positives: 1288\tFalse positives:  656\tFalse negatives:  712\tTrue negatives: 12344\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/pickle\n",
    "\n",
    "\"\"\" a basic script for importing student's POI identifier,\n",
    "    and checking the results that they get from it \n",
    " \n",
    "    requires that the algorithm, dataset, and features list\n",
    "    be written to my_classifier.pkl, my_dataset.pkl, and\n",
    "    my_feature_list.pkl, respectively\n",
    "\n",
    "    that process should happen at the end of poi_id.py\n",
    "\"\"\"\n",
    "\n",
    "import pickle\n",
    "import sys\n",
    "from sklearn.cross_validation import StratifiedShuffleSplit\n",
    "sys.path.append(\"../tools/\")\n",
    "from feature_format import featureFormat, targetFeatureSplit\n",
    "\n",
    "PERF_FORMAT_STRING = \"\\\n",
    "\\tAccuracy: {:>0.{display_precision}f}\\tPrecision: {:>0.{display_precision}f}\\t\\\n",
    "Recall: {:>0.{display_precision}f}\\tF1: {:>0.{display_precision}f}\\tF2: {:>0.{display_precision}f}\"\n",
    "RESULTS_FORMAT_STRING = \"\\tTotal predictions: {:4d}\\tTrue positives: {:4d}\\tFalse positives: {:4d}\\\n",
    "\\tFalse negatives: {:4d}\\tTrue negatives: {:4d}\"\n",
    "\n",
    "def test_classifier(clf, dataset, feature_list, folds = 1000):\n",
    "    data = featureFormat(dataset, feature_list, sort_keys = True)\n",
    "    labels, features = targetFeatureSplit(data)\n",
    "    cv = StratifiedShuffleSplit(labels, folds, random_state = 42)\n",
    "    true_negatives = 0\n",
    "    false_negatives = 0\n",
    "    true_positives = 0\n",
    "    false_positives = 0\n",
    "    for train_idx, test_idx in cv: \n",
    "        features_train = []\n",
    "        features_test  = []\n",
    "        labels_train   = []\n",
    "        labels_test    = []\n",
    "        for ii in train_idx:\n",
    "            features_train.append( features[ii] )\n",
    "            labels_train.append( labels[ii] )\n",
    "        for jj in test_idx:\n",
    "            features_test.append( features[jj] )\n",
    "            labels_test.append( labels[jj] )\n",
    "        \n",
    "        ### fit the classifier using training set, and test on test set\n",
    "        clf.fit(features_train, labels_train)\n",
    "        predictions = clf.predict(features_test)\n",
    "        for prediction, truth in zip(predictions, labels_test):\n",
    "            if prediction == 0 and truth == 0:\n",
    "                true_negatives += 1\n",
    "            elif prediction == 0 and truth == 1:\n",
    "                false_negatives += 1\n",
    "            elif prediction == 1 and truth == 0:\n",
    "                false_positives += 1\n",
    "            elif prediction == 1 and truth == 1:\n",
    "                true_positives += 1\n",
    "            else:\n",
    "                print \"Warning: Found a predicted label not == 0 or 1.\"\n",
    "                print \"All predictions should take value 0 or 1.\"\n",
    "                print \"Evaluating performance for processed predictions:\"\n",
    "                break\n",
    "    try:\n",
    "        total_predictions = true_negatives + false_negatives + false_positives + true_positives\n",
    "        accuracy = 1.0*(true_positives + true_negatives)/total_predictions\n",
    "        precision = 1.0*true_positives/(true_positives+false_positives)\n",
    "        recall = 1.0*true_positives/(true_positives+false_negatives)\n",
    "        f1 = 2.0 * true_positives/(2*true_positives + false_positives+false_negatives)\n",
    "        f2 = (1+2.0*2.0) * precision*recall/(4*precision + recall)\n",
    "        print clf\n",
    "        print PERF_FORMAT_STRING.format(accuracy, precision, recall, f1, f2, display_precision = 5)\n",
    "        print RESULTS_FORMAT_STRING.format(total_predictions, true_positives, false_positives, false_negatives, true_negatives)\n",
    "        print \"\"\n",
    "    except:\n",
    "        print \"Got a divide by zero when trying out:\", clf\n",
    "        print \"Precision or recall may be undefined due to a lack of true positive predicitons.\"\n",
    "\n",
    "CLF_PICKLE_FILENAME = \"my_classifier.pkl\"\n",
    "DATASET_PICKLE_FILENAME = \"my_dataset.pkl\"\n",
    "FEATURE_LIST_FILENAME = \"my_feature_list.pkl\"\n",
    "\n",
    "def dump_classifier_and_data(clf, dataset, feature_list):\n",
    "    with open(CLF_PICKLE_FILENAME, \"w\") as clf_outfile:\n",
    "        pickle.dump(clf, clf_outfile)\n",
    "    with open(DATASET_PICKLE_FILENAME, \"w\") as dataset_outfile:\n",
    "        pickle.dump(dataset, dataset_outfile)\n",
    "    with open(FEATURE_LIST_FILENAME, \"w\") as featurelist_outfile:\n",
    "        pickle.dump(feature_list, featurelist_outfile)\n",
    "\n",
    "def load_classifier_and_data():\n",
    "    with open(CLF_PICKLE_FILENAME, \"r\") as clf_infile:\n",
    "        clf = pickle.load(clf_infile)\n",
    "    with open(DATASET_PICKLE_FILENAME, \"r\") as dataset_infile:\n",
    "        dataset = pickle.load(dataset_infile)\n",
    "    with open(FEATURE_LIST_FILENAME, \"r\") as featurelist_infile:\n",
    "        feature_list = pickle.load(featurelist_infile)\n",
    "    return clf, dataset, feature_list\n",
    "\n",
    "def main():\n",
    "    ### load up student's classifier, dataset, and feature_list\n",
    "    clf, dataset, feature_list = load_classifier_and_data()\n",
    "    ### Run testing script\n",
    "    test_classifier(clf, dataset, feature_list)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration\n",
    "\n",
    "\n",
    "20 of the 21 features have missing values (represented as \"NaN\"), with the exception being the \"poi\" feature.\n",
    "\n",
    "The missing financial features are imputed by featureFormat to zero (0). Imputing to zero makes sense for these features because we have a reasonably complete financial picture through the FindLaw \"Payments to Insiders\" document. I am assuming that if a feature has a dash ('-'), like several 'bonus' rows do, that means it is zero. That isn't an unreasonable assumption since there are no actual zeros in that document and the dashes take their place.\n",
    "\n",
    "I imputed the missing email features to each feature's median. Imputing to zero doesn't make sense in this case because the email data appears incomplete. 60 of the 146 employees in the dataset have \"NaN\" for all of their email features. A missing feature likely means we couldn't find the data, rather than the value is zero. Though this introduces some bias, we are at the whim of the dataset and imputing to the mean is a fine option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/python\n",
    "\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_classif\n",
    "from sklearn.decomposition import PCA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "sys.path.append(\"../tools\")\n",
    "from feature_format import featureFormat, targetFeatureSplit\n",
    "import tester"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "features_list = ['poi',\n",
    "                'salary',\n",
    "                'bonus', \n",
    "                'long_term_incentive', \n",
    "                'deferred_income', \n",
    "                'deferral_payments',\n",
    "                'loan_advances', \n",
    "                'other',\n",
    "                'expenses', \n",
    "                'director_fees',\n",
    "                'total_payments',\n",
    "                'exercised_stock_options',\n",
    "                'restricted_stock',\n",
    "                'restricted_stock_deferred',\n",
    "                'total_stock_value',\n",
    "                'to_messages',\n",
    "                'from_messages',\n",
    "                'from_this_person_to_poi',\n",
    "                'from_poi_to_this_person']\n",
    "\n",
    " # features_list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 146 entries, ALLEN PHILLIP K to YEAP SOON\n",
      "Data columns (total 19 columns):\n",
      "poi                          146 non-null bool\n",
      "salary                       95 non-null float64\n",
      "bonus                        82 non-null float64\n",
      "long_term_incentive          66 non-null float64\n",
      "deferred_income              49 non-null float64\n",
      "deferral_payments            39 non-null float64\n",
      "loan_advances                4 non-null float64\n",
      "other                        93 non-null float64\n",
      "expenses                     95 non-null float64\n",
      "director_fees                17 non-null float64\n",
      "total_payments               125 non-null float64\n",
      "exercised_stock_options      102 non-null float64\n",
      "restricted_stock             110 non-null float64\n",
      "restricted_stock_deferred    18 non-null float64\n",
      "total_stock_value            126 non-null float64\n",
      "to_messages                  86 non-null float64\n",
      "from_messages                86 non-null float64\n",
      "from_this_person_to_poi      86 non-null float64\n",
      "from_poi_to_this_person      86 non-null float64\n",
      "dtypes: bool(1), float64(18)\n",
      "memory usage: 21.8+ KB\n"
     ]
    }
   ],
   "source": [
    "### Load the dictionary containing the dataset\n",
    "with open(\"final_project_dataset.pkl\", \"r\")  as data_file:\n",
    "    data_dict = pickle.load(data_file)\n",
    "\n",
    "# Transform data from dictionary to the Pandas DataFrame\n",
    "df = pd.DataFrame.from_dict(data_dict, orient = \"index\")\n",
    "\n",
    "#Order columns in DataFrame, exclude email column\n",
    "df = df[features_list]\n",
    "df = df.replace('NaN', np.nan)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "poi / non_poi split\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "non-POI    128\n",
       "POI         18\n",
       "Name: poi, dtype: int64"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#split of POI and non-POI in the dataset\n",
    "poi_non_poi = df.poi.value_counts()\n",
    "poi_non_poi.index = [\"non-POI\", \"POI\"]\n",
    "print \"poi / non_poi split\"\n",
    "poi_non_poi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amount of NaN values in the dataset:  1263\n"
     ]
    }
   ],
   "source": [
    "## Amount of NA's in the dataset\n",
    "\n",
    "print \"Amount of NaN values in the dataset: \", df.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the financial dat from FindLaw , NaN's represents values of ) but not the missing values'. Thats why I am going to replace all NaNs with 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Replacing \"NaN\" in financial features with 0\n",
    "\n",
    "df.ix[:,:15] = df.ix[:,:15].fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NaNs values in email features means the information is missing. Thus, I will split the data into 2 classes POI/non-POI for the sake of missing values imputation. I will use the median of each class for NA's imputation in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Replacing NAN's\n",
    "email_features = [\"to_messages\", \"from_messages\", \"from_this_person_to_poi\", \"from_poi_to_this_person\"]\n",
    "\n",
    "imp = Imputer(missing_values = \"NaN\", strategy = \"median\", axis = 0)\n",
    "\n",
    "\n",
    "### Impute missing values of email features \n",
    "df.loc[df[df.poi == 1].index, email_features] = imp.fit_transform(df[email_features][df.poi == 1])\n",
    "df.loc[df[df.poi == 0].index, email_features] = imp.fit_transform(df[email_features][df.poi == 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, I will check the accuracy of the financial dataset by summing up the payment features and comparing it with the total_payemnet feature. Furthermore, I will also comapare stock features with the total_stock_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>poi</th>\n",
       "      <th>salary</th>\n",
       "      <th>bonus</th>\n",
       "      <th>long_term_incentive</th>\n",
       "      <th>deferred_income</th>\n",
       "      <th>deferral_payments</th>\n",
       "      <th>loan_advances</th>\n",
       "      <th>other</th>\n",
       "      <th>expenses</th>\n",
       "      <th>director_fees</th>\n",
       "      <th>total_payments</th>\n",
       "      <th>exercised_stock_options</th>\n",
       "      <th>restricted_stock</th>\n",
       "      <th>restricted_stock_deferred</th>\n",
       "      <th>total_stock_value</th>\n",
       "      <th>to_messages</th>\n",
       "      <th>from_messages</th>\n",
       "      <th>from_this_person_to_poi</th>\n",
       "      <th>from_poi_to_this_person</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>BELFER ROBERT</th>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-102500.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3285.0</td>\n",
       "      <td>102500.0</td>\n",
       "      <td>3285.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>44093.0</td>\n",
       "      <td>-44093.0</td>\n",
       "      <td>944.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>26.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BHATNAGAR SANJAY</th>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>137864.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>137864.0</td>\n",
       "      <td>15456290.0</td>\n",
       "      <td>2604490.0</td>\n",
       "      <td>-2604490.0</td>\n",
       "      <td>15456290.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>523.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    poi  salary  bonus  long_term_incentive  deferred_income  \\\n",
       "BELFER ROBERT     False     0.0    0.0                  0.0              0.0   \n",
       "BHATNAGAR SANJAY  False     0.0    0.0                  0.0              0.0   \n",
       "\n",
       "                  deferral_payments  loan_advances     other  expenses  \\\n",
       "BELFER ROBERT             -102500.0            0.0       0.0       0.0   \n",
       "BHATNAGAR SANJAY                0.0            0.0  137864.0       0.0   \n",
       "\n",
       "                  director_fees  total_payments  exercised_stock_options  \\\n",
       "BELFER ROBERT            3285.0        102500.0                   3285.0   \n",
       "BHATNAGAR SANJAY       137864.0      15456290.0                2604490.0   \n",
       "\n",
       "                  restricted_stock  restricted_stock_deferred  \\\n",
       "BELFER ROBERT                  0.0                    44093.0   \n",
       "BHATNAGAR SANJAY        -2604490.0                 15456290.0   \n",
       "\n",
       "                  total_stock_value  to_messages  from_messages  \\\n",
       "BELFER ROBERT              -44093.0        944.0           41.0   \n",
       "BHATNAGAR SANJAY                0.0        523.0           29.0   \n",
       "\n",
       "                  from_this_person_to_poi  from_poi_to_this_person  \n",
       "BELFER ROBERT                         6.0                     26.5  \n",
       "BHATNAGAR SANJAY                      1.0                      0.0  "
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### check data: summing payments features and comparing them with total_payments\n",
    "payments = [\"salary\",\n",
    "            \"bonus\", \n",
    "            \"long_term_incentive\", \n",
    "            \"deferred_income\", \n",
    "            \"deferral_payments\",\n",
    "            \"loan_advances\", \n",
    "            \"other\",\n",
    "            \"expenses\", \n",
    "            \"director_fees\"]\n",
    "\n",
    "df[df[payments].sum(axis = \"columns\") != df.total_payments]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>poi</th>\n",
       "      <th>salary</th>\n",
       "      <th>bonus</th>\n",
       "      <th>long_term_incentive</th>\n",
       "      <th>deferred_income</th>\n",
       "      <th>deferral_payments</th>\n",
       "      <th>loan_advances</th>\n",
       "      <th>other</th>\n",
       "      <th>expenses</th>\n",
       "      <th>director_fees</th>\n",
       "      <th>total_payments</th>\n",
       "      <th>exercised_stock_options</th>\n",
       "      <th>restricted_stock</th>\n",
       "      <th>restricted_stock_deferred</th>\n",
       "      <th>total_stock_value</th>\n",
       "      <th>to_messages</th>\n",
       "      <th>from_messages</th>\n",
       "      <th>from_this_person_to_poi</th>\n",
       "      <th>from_poi_to_this_person</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>BELFER ROBERT</th>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-102500.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3285.0</td>\n",
       "      <td>102500.0</td>\n",
       "      <td>3285.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>44093.0</td>\n",
       "      <td>-44093.0</td>\n",
       "      <td>944.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>26.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BHATNAGAR SANJAY</th>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>137864.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>137864.0</td>\n",
       "      <td>15456290.0</td>\n",
       "      <td>2604490.0</td>\n",
       "      <td>-2604490.0</td>\n",
       "      <td>15456290.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>523.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    poi  salary  bonus  long_term_incentive  deferred_income  \\\n",
       "BELFER ROBERT     False     0.0    0.0                  0.0              0.0   \n",
       "BHATNAGAR SANJAY  False     0.0    0.0                  0.0              0.0   \n",
       "\n",
       "                  deferral_payments  loan_advances     other  expenses  \\\n",
       "BELFER ROBERT             -102500.0            0.0       0.0       0.0   \n",
       "BHATNAGAR SANJAY                0.0            0.0  137864.0       0.0   \n",
       "\n",
       "                  director_fees  total_payments  exercised_stock_options  \\\n",
       "BELFER ROBERT            3285.0        102500.0                   3285.0   \n",
       "BHATNAGAR SANJAY       137864.0      15456290.0                2604490.0   \n",
       "\n",
       "                  restricted_stock  restricted_stock_deferred  \\\n",
       "BELFER ROBERT                  0.0                    44093.0   \n",
       "BHATNAGAR SANJAY        -2604490.0                 15456290.0   \n",
       "\n",
       "                  total_stock_value  to_messages  from_messages  \\\n",
       "BELFER ROBERT              -44093.0        944.0           41.0   \n",
       "BHATNAGAR SANJAY                0.0        523.0           29.0   \n",
       "\n",
       "                  from_this_person_to_poi  from_poi_to_this_person  \n",
       "BELFER ROBERT                         6.0                     26.5  \n",
       "BHATNAGAR SANJAY                      1.0                      0.0  "
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stock_value = [\"exercised_stock_options\",\n",
    "              \"restricted_stock\",\n",
    "              \"restricted_stock_deferred\"]\n",
    "\n",
    "df[df[stock_value].sum(axis = \"columns\") != df.total_stock_value]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems two samples have mistakes in the data entry. Thus, I am going to rectify the situation by correcting the worngly entered data. Ultimately, everything in the data set will be cross checked (i.e. empty DataFrame mean no samples with mistakes in the dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>poi</th>\n",
       "      <th>salary</th>\n",
       "      <th>bonus</th>\n",
       "      <th>long_term_incentive</th>\n",
       "      <th>deferred_income</th>\n",
       "      <th>deferral_payments</th>\n",
       "      <th>loan_advances</th>\n",
       "      <th>other</th>\n",
       "      <th>expenses</th>\n",
       "      <th>director_fees</th>\n",
       "      <th>total_payments</th>\n",
       "      <th>exercised_stock_options</th>\n",
       "      <th>restricted_stock</th>\n",
       "      <th>restricted_stock_deferred</th>\n",
       "      <th>total_stock_value</th>\n",
       "      <th>to_messages</th>\n",
       "      <th>from_messages</th>\n",
       "      <th>from_this_person_to_poi</th>\n",
       "      <th>from_poi_to_this_person</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [poi, salary, bonus, long_term_incentive, deferred_income, deferral_payments, loan_advances, other, expenses, director_fees, total_payments, exercised_stock_options, restricted_stock, restricted_stock_deferred, total_stock_value, to_messages, from_messages, from_this_person_to_poi, from_poi_to_this_person]\n",
       "Index: []"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.ix['BELFER ROBERT','total_payments'] = 3285\n",
    "df.ix['BELFER ROBERT','deferral_payments'] = 0\n",
    "df.ix['BELFER ROBERT','restricted_stock'] = 44093\n",
    "df.ix['BELFER ROBERT','restricted_stock_deferred'] = -44093\n",
    "df.ix['BELFER ROBERT','total_stock_value'] = 0\n",
    "df.ix['BELFER ROBERT','director_fees'] = 102500\n",
    "df.ix['BELFER ROBERT','deferred_income'] = -102500\n",
    "df.ix['BELFER ROBERT','exercised_stock_options'] = 0\n",
    "df.ix['BELFER ROBERT','expenses'] = 3285\n",
    "df.ix['BELFER ROBERT',]\n",
    "df.ix['BHATNAGAR SANJAY','expenses'] = 137864\n",
    "df.ix['BHATNAGAR SANJAY','total_payments'] = 137864\n",
    "df.ix['BHATNAGAR SANJAY','exercised_stock_options'] = 1.54563e+07\n",
    "df.ix['BHATNAGAR SANJAY','restricted_stock'] = 2.60449e+06\n",
    "df.ix['BHATNAGAR SANJAY','restricted_stock_deferred'] = -2.60449e+06\n",
    "df.ix['BHATNAGAR SANJAY','other'] = 0\n",
    "df.ix['BHATNAGAR SANJAY','director_fees'] = 0\n",
    "df.ix['BHATNAGAR SANJAY','total_stock_value'] = 1.54563e+07\n",
    "df.ix['BHATNAGAR SANJAY']\n",
    "\n",
    "df[df[payments].sum(axis='columns') != df.total_payments]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>poi</th>\n",
       "      <th>salary</th>\n",
       "      <th>bonus</th>\n",
       "      <th>long_term_incentive</th>\n",
       "      <th>deferred_income</th>\n",
       "      <th>deferral_payments</th>\n",
       "      <th>loan_advances</th>\n",
       "      <th>other</th>\n",
       "      <th>expenses</th>\n",
       "      <th>director_fees</th>\n",
       "      <th>total_payments</th>\n",
       "      <th>exercised_stock_options</th>\n",
       "      <th>restricted_stock</th>\n",
       "      <th>restricted_stock_deferred</th>\n",
       "      <th>total_stock_value</th>\n",
       "      <th>to_messages</th>\n",
       "      <th>from_messages</th>\n",
       "      <th>from_this_person_to_poi</th>\n",
       "      <th>from_poi_to_this_person</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [poi, salary, bonus, long_term_incentive, deferred_income, deferral_payments, loan_advances, other, expenses, director_fees, total_payments, exercised_stock_options, restricted_stock, restricted_stock_deferred, total_stock_value, to_messages, from_messages, from_this_person_to_poi, from_poi_to_this_person]\n",
       "Index: []"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df[stock_value].sum(axis='columns') != df.total_stock_value]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outlier Investigation \n",
    "\n",
    "In the previous step, datset was cleaned for missing values as well typos. Next, I would explore the data for plausible outliers. This will be accomplished by using descriptive statistics. We know that desciptive statitics determiene the distribution as the values which are higher than **Q2 + 1.5IQR** or **Q2 - 1.5IQR**; where Q2 is median of the distribution, IQR is interquartile range. I will caculate the sum of the outlier variables for each person and sort them in descending order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th># of outliers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>TOTAL</th>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LAY KENNETH L</th>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FREVERT MARK A</th>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WHALLEY LAWRENCE G</th>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SKILLING JEFFREY K</th>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LAVORATO JOHN J</th>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MCMAHON JEFFREY</th>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    # of outliers\n",
       "TOTAL                          12\n",
       "LAY KENNETH L                  12\n",
       "FREVERT MARK A                 12\n",
       "WHALLEY LAWRENCE G             11\n",
       "SKILLING JEFFREY K             11\n",
       "LAVORATO JOHN J                 9\n",
       "MCMAHON JEFFREY                 8"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outliers = df.quantile (.5) + 1.5 * (df.quantile (.75) - df.quantile (.25))\n",
    "\n",
    "pd.DataFrame((df[1:] > outliers[1:]).sum(axis = 1), columns = [\"# of outliers\"]).\\\n",
    "    sort_values(\"# of outliers\", ascending = [0]).head(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our data set is really small, so I'm going to consider just 5% of the samples with most number of outlier variables:\n",
    "\n",
    "* The first value is 'TOTAL' which is the total value of financial payments from the FindLaw data. As it's doesn't make any sence for our solution, I'm going to exclude it from the data set.\n",
    "* Kenneth Lay and Jeffrey Skilling are very well known persons from ENRON - I should keep them as they represent anomalies but not the outliers.\n",
    "* Mark Frevert and Lawrence Whalley are not so very well known but top managers of the Enron who also represent valuable examples for the model - I'm also going to keep them in the data set.\n",
    "* John Lavorato is not very well known person as far as I've searched in the internet. I don't think he represents a valid point and exclude him.\n",
    "* Jeffrey Mcmahon is the former treasurer who worked before guilty Ben Glisan. I would exclude him from the data set as he worked before the guilty treasurer and might add some confusion to the model.\n",
    "\n",
    "From considered 7 persons I've ended up with excluding 3 of them (1 typo 'TOTAL' and 2 persons)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "df_norm = df[features_list]\n",
    "df_norm = scaler.fit_transform(df_norm.ix[:,1:])\n",
    "\n",
    "clf = GaussianNB()\n",
    "\n",
    "features_list2 = ['poi'] + range(8)\n",
    "\n",
    "my_dataset = pd.DataFrame(SelectKBest(f_classif, k = 8).fit_transform(df_norm, df.poi), index = df.index)\n",
    "my_dataset.insert(0, \"poi\", df.poi)\n",
    "my_dataset = my_dataset.to_dict(orient = \"index\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GaussianNB(priors=None)\n",
      "\tAccuracy: 0.33760\tPrecision: 0.14848\tRecall: 0.83800\tF1: 0.25226\tF2: 0.43447\n",
      "\tTotal predictions: 15000\tTrue positives: 1676\tFalse positives: 9612\tFalse negatives:  324\tTrue negatives: 3388\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dump_classifier_and_data(clf, my_dataset, features_list2)\n",
    "\n",
    "tester.main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### exclude 3 outliers from the data set\n",
    "\n",
    "df = df.drop(['TOTAL', 'LAVORATO JOHN J', 'MCMAHON JEFFREY'],0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimise Feature Selection/Engineering\n",
    "\n",
    "Machine learning uses so called features (i.e. variables or attributes) to generate predictive models. Using a suitable combination of features is essential for obtaining high precision and accuracy. Because too many (unspecific) features pose the problem of overfitting the model, we generally want to restrict the features in our models to those, that are most relevant for the response variable we want to predict. Using as few features as possible will also reduce the complexity of our models, which means it needs less time and computer power to run and is easier to understand.\n",
    "\n",
    "There are several ways to identify how much each feature contributes to the model and to restrict the number of selected features. Here, I am going to examine the effect of feature selection via\n",
    "\n",
    "  * Decision Tree C lassifier , incl. choosing the features with features importance attribute and tuning the model.\n",
    "\n",
    "or Random Forest models.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "\n",
    "For both strategies I've tried to create new features as a fraction of almost all financial variables (f.ex. fractional bonus as fraction of bonus to total_payments, etc.). Logic behind email feature creation was to check the fraction of emails, sent to POI, to all sent emails; emails, received from POI, to all received emails.\n",
    "I've end up with using one new feature fraction_to_POI:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Create additional feature : fraction of person's email to POI to all sent messages\n",
    "\n",
    "df[\"fraction_to_poi\"] = df[\"from_this_person_to_poi\"]/df[\"from_messages\"]\n",
    "\n",
    "### Clean all \"inf\" values which we got if the perosn from_messages = 0\n",
    "df = df.replace(\"inf\", 0)\n",
    "#df.head(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second strategy showed significantly better results so the rest of the project I'm going to concentrate on it. Decision tree doesn't require me to do any feature scaling so I've skipped this step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection \n",
    "\n",
    "On the feature selection step I've fitted my DecisionTreeClassifier with all features and as a result received number of features with non-null feature importance, sorted by importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fraction_to_poi', 0.35824390243902443]\n",
      "['expenses', 0.26431889023871075]\n",
      "['to_messages', 0.16306330961503368]\n",
      "['other', 0.084740740740740714]\n",
      "['deferred_income', 0.070617283950617254]\n",
      "['from_poi_to_this_person', 0.059015873015873015]\n"
     ]
    }
   ],
   "source": [
    "### Decision tree using features with non-null importance\n",
    "clf = DecisionTreeClassifier(random_state = 75)\n",
    "clf.fit(df.ix[:, 1:], df.ix[:, :1])\n",
    "\n",
    "### Show the feature with non-null importance\n",
    "### Sort, create feature list of the feature model \n",
    "feature_importance = []\n",
    "for i in range (len(clf.feature_importances_)):\n",
    "    if clf.feature_importances_[i] > 0:\n",
    "        feature_importance.append(([df.columns[i + 1], clf.feature_importances_[i]]))\n",
    "feature_importance.sort(key = lambda x: x[1], reverse = True)\n",
    "\n",
    "for f_i in feature_importance:\n",
    "    print f_i\n",
    "\n",
    "features_list = [x[0] for x in feature_importance]\n",
    "features_list.insert(0, \"poi\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to feature_importances attribute of the classifier, just created fraction_to_poi feature has the highest importance for the model. The number of features used for the model may cause different results. Later on the algorithm tuning step I'm going to re-iterate the step of choosing features with non-null importance so the number of them will be changed.\n",
    "\n",
    "I'm using random state equal to 75 in decision tree and random forest to be able to represent the results. The exact value was manually chosen for better performance of decision tree classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Building \n",
    "\n",
    "I focused on four algorithms, with parameter tuning incorporated into algorithm selection (i.e. parameters tuned for more than one algorithm, and best algorithm-tune combination selected for final analysis). These algorithms were:\n",
    "\n",
    "* Decision Tree Classifier\n",
    "* AdaBoost Classifier\n",
    "* Random Forest\n",
    "* GaussianNB\n",
    "\n",
    "For decision tree and random forest I've selected just features with non-null importance based on clf.features_importances__. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model No. 1 -  Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Decision Tree Classifier with standard parametres \n",
    "\n",
    "clf = DecisionTreeClassifier (random_state = 75)\n",
    "my_dataset = df[features_list].to_dict(orient = \"index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=75, splitter='best')\n",
      "\tAccuracy: 0.90880\tPrecision: 0.66255\tRecall: 0.64400\tF1: 0.65314\tF2: 0.64763\n",
      "\tTotal predictions: 15000\tTrue positives: 1288\tFalse positives:  656\tFalse negatives:  712\tTrue negatives: 12344\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tester.dump_classifier_and_data(clf, my_dataset, features_list)\n",
    "tester.main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model No. 2 -  AdaBoost Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### AdaBoost Classifier with standard parameters\n",
    "\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "clf = AdaBoostClassifier(random_state = 75)\n",
    "\n",
    "my_dataset = df[features_list].to_dict(orient = \"index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None,\n",
      "          learning_rate=1.0, n_estimators=50, random_state=75)\n",
      "\tAccuracy: 0.91680\tPrecision: 0.69936\tRecall: 0.65950\tF1: 0.67885\tF2: 0.66710\n",
      "\tTotal predictions: 15000\tTrue positives: 1319\tFalse positives:  567\tFalse negatives:  681\tTrue negatives: 12433\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tester.dump_classifier_and_data(clf, my_dataset, features_list)\n",
    "tester.main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model No. 3 -  Random Forest "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Random Forest with standard parameters\n",
    "clf = RandomForestClassifier(random_state = 75)\n",
    "clf.fit(df.ix[:,1:], np.ravel(df.ix[:,:1]))\n",
    "\n",
    "# selecting the features with non null importance, sorting and creating features_list for the model\n",
    "features_importance = []\n",
    "for i in range(len(clf.feature_importances_)):\n",
    "    if clf.feature_importances_[i] > 0:\n",
    "        features_importance.append([df.columns[i+1], clf.feature_importances_[i]])\n",
    "features_importance.sort(key=lambda x: x[1], reverse = True)\n",
    "features_list = [x[0] for x in features_importance]\n",
    "features_list.insert(0, 'poi')\n",
    "\n",
    "# number of features for best result was found iteratively\n",
    "features_list2 = features_list[:11]\n",
    "my_dataset = df[features_list2].to_dict(orient = 'index')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            n_estimators=10, n_jobs=1, oob_score=False, random_state=75,\n",
      "            verbose=0, warm_start=False)\n",
      "\tAccuracy: 0.89780\tPrecision: 0.70322\tRecall: 0.40400\tF1: 0.51318\tF2: 0.44158\n",
      "\tTotal predictions: 15000\tTrue positives:  808\tFalse positives:  341\tFalse negatives: 1192\tTrue negatives: 12659\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tester.dump_classifier_and_data(clf, my_dataset, features_list2)\n",
    "tester.main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model No. 4 -  GaussianNB  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# GaussianNB with feature standartization, selection, PCA\n",
    "\n",
    "clf = GaussianNB()\n",
    "\n",
    "# data set standartization\n",
    "scaler = StandardScaler()\n",
    "df_norm = df[features_list]\n",
    "df_norm = scaler.fit_transform(df_norm.ix[:,1:])\n",
    "\n",
    "# feature selection\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_classif\n",
    "features_list2 = ['poi']+range(3)\n",
    "my_dataset = pd.DataFrame(SelectKBest(f_classif, k=8).fit_transform(df_norm, df.poi), index = df.index)\n",
    "\n",
    "#PCA\n",
    "pca = PCA(n_components=3)\n",
    "my_dataset2 = pd.DataFrame(pca.fit_transform(my_dataset),  index=df.index)\n",
    "my_dataset2.insert(0, \"poi\", df.poi)\n",
    "my_dataset2 = my_dataset2.to_dict(orient = 'index')  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GaussianNB(priors=None)\n",
      "\tAccuracy: 0.86447\tPrecision: 0.49065\tRecall: 0.43300\tF1: 0.46003\tF2: 0.44342\n",
      "\tTotal predictions: 15000\tTrue positives:  866\tFalse positives:  899\tFalse negatives: 1134\tTrue negatives: 12101\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dump_classifier_and_data(clf, my_dataset2, features_list2)\n",
    "tester.main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Though I used the classfiication report for quick checks, I used tester.py's evaluation metrics to make sure I would get precision and recall above 0.3 for the Udacity grading system. Here is how each performed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Decision Tree Classifier</th>\n",
       "      <td>0.90880</td>\n",
       "      <td>0.66255</td>\n",
       "      <td>0.6440</td>\n",
       "      <td>0.65314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AdaBoost</th>\n",
       "      <td>0.89120</td>\n",
       "      <td>0.60372</td>\n",
       "      <td>0.5355</td>\n",
       "      <td>0.56757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random Forest</th>\n",
       "      <td>0.89780</td>\n",
       "      <td>0.70322</td>\n",
       "      <td>0.4040</td>\n",
       "      <td>0.51318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes</th>\n",
       "      <td>0.86447</td>\n",
       "      <td>0.49065</td>\n",
       "      <td>0.4330</td>\n",
       "      <td>0.46003</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          Accuracy  Precision  Recall       F1\n",
       "Decision Tree Classifier   0.90880    0.66255  0.6440  0.65314\n",
       "AdaBoost                   0.89120    0.60372  0.5355  0.56757\n",
       "Random Forest              0.89780    0.70322  0.4040  0.51318\n",
       "Gaussian Naive Bayes       0.86447    0.49065  0.4330  0.46003"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame([[0.90880, 0.66255, 0.64400, 0.65314],\n",
    "              [0.89120, 0.60372, 0.53550, 0.56757],\n",
    "              [0.89780, 0.70322, 0.40400, 0.51318],\n",
    "              [0.86447, 0.49065, 0.43300, 0.46003]],\n",
    "             columns = ['Accuracy','Precision', 'Recall', 'F1'], \n",
    "             index = ['Decision Tree Classifier', 'AdaBoost', 'Random Forest', 'Gaussian Naive Bayes'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorithm Tuning \n",
    "\n",
    "Tuning the parameters of an algorithm means adjusting the parameters in a certain way to achieve optimal algorithm performance. There are a variety of \"certain ways\" (e.g. a manual guess-and-check method or automatically with GridSearchCV) and \"algorithm performance\" can be measured in a variety of ways (e.g. accuracy, precision, or recall). If you don't tune the algorithm well, performance may suffer. The data won't be \"learned\" well and you won't be able to successfully make predictions on new data.\n",
    "\n",
    "Bias-variance tradeoff is one of the key dilema in machine learning. High bias algorithms has no capacity to learn, high variance algorithms react poorly in case they didn't see such data before. Predictive model should be tuned to achieve compromise. The process of changing the parameteres of algorithms is algorithm tuning and it lets us find the golden mean and best result. For the chosen decision tree classifier for example, I tried out multiple different parameter values for each of the following parameters (with the optimal combination bolded). I used Stratified Shuffle Split cross validation to guard against bias introduced by the potential underrepresentation of classes (i.e. POIs).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Decision Tree\n",
    "Accuracy: 0.90880\t\n",
    "Precision: 0.66255\t\n",
    "Recall: 0.64400\t\n",
    "F1: 0.65314\n",
    "\n",
    "* Parameters\n",
    "  + class_weight = None \n",
    "  + criterion = 'gini'\n",
    "  + max_depth = None\n",
    "  + max_features = None \n",
    "  + max_leaf_nodes=None\n",
    "  + min_impurity_split = 1e-07 \n",
    "  + min_samples_leaf = 1\n",
    "  + min_samples_split = 2 \n",
    "  + min_weight_fraction_leaf = 0.0\n",
    "  + presort=False\n",
    "  + random_state = 75\n",
    "  + splitter = 'best'\n",
    "\n",
    "######  AdaBoost \n",
    "Accuracy: 0.89120\t\n",
    "Precision: 0.60372\t\n",
    "Recall: 0.53550\t\n",
    "F1: 0.56757\n",
    "\n",
    "* Parameters\n",
    "  + algorithm = 'SAMME.R'\n",
    "  + base_estimator = None\n",
    "  + learning_rate = 1.0 \n",
    "  + n_estimators = 50 \n",
    "  + random_state = 75\n",
    "\n",
    "###### Random Forest \n",
    "Accuracy: 0.89780\n",
    "Precision: 0.70322\n",
    "Recall: 0.40400\n",
    "F1: 0.51318\n",
    "\n",
    "* Parameters\n",
    "  + bootstrap = True \n",
    "  + class_weight = None\n",
    "  + criterion = 'gini'\n",
    "  + max_depth = None\n",
    "  + max_features = 'auto'\n",
    "  + max_leaf_nodes = None,\n",
    "  + min_impurity_split = 1e-07\n",
    "  + min_samples_leaf = 1\n",
    "  + min_samples_split = 2\n",
    "  + min_weight_fraction_leaf = 0.0\n",
    "  + n_estimators=10\n",
    "  + n_jobs=1\n",
    "  + oob_score = False\n",
    "  + random_state = 75\n",
    "  + verbose = 0\n",
    "  + warm_start = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf = DecisionTreeClassifier(criterion = \"entropy\",\n",
    "                            min_samples_split = 19,\n",
    "                            random_state = 75,\n",
    "                            min_samples_leaf = 6,\n",
    "                            max_depth = 3)\n",
    "\n",
    "clf.fit(df.ix[:,1:], df.poi)\n",
    "\n",
    "\n",
    "# show the features with non null importance, sorted and create features_list of features for the model\n",
    "features_importance = []\n",
    "for i in range(len(clf.feature_importances_)):\n",
    "    if clf.feature_importances_[i] > 0:\n",
    "        features_importance.append([df.columns[i+1], clf.feature_importances_[i]])\n",
    "features_importance.sort(key=lambda x: x[1], reverse = True)\n",
    "\n",
    "features_list = [x[0] for x in features_importance]\n",
    "features_list.insert(0, 'poi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=3,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=6,\n",
      "            min_samples_split=19, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=75, splitter='best')\n",
      "\tAccuracy: 0.93673\tPrecision: 0.83238\tRecall: 0.65800\tF1: 0.73499\tF2: 0.68678\n",
      "\tTotal predictions: 15000\tTrue positives: 1316\tFalse positives:  265\tFalse negatives:  684\tTrue negatives: 12735\n",
      "\n"
     ]
    }
   ],
   "source": [
    "my_dataset = df[features_list].to_dict(orient = 'index')\n",
    "tester.dump_classifier_and_data(clf, my_dataset, features_list)\n",
    "tester.main() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation and Valiadation \n",
    "### Usage of Evaluation Metrics\n",
    "\n",
    "In the project I've used F1 score as key measure of algorithms' accuracy. It considers both the precision and the recall of the test to compute the score.\n",
    "* Precision is the ability of the classifier not label as positive sample that is negative.\n",
    "* Recall is the ability of the classifier to find all positive samples.\n",
    "* The F1 score can be interpreted as a weighted average of the precision and recall, where an F1 score reaches its best value at 1 and worst at 0.\n",
    "\n",
    "My tuned decision tree classifier showed precision0.93673 and recall 0.65800 with the resulting F1 score 0.73499. I can explain it as 93.68% of the called POI are POI and 65.80% of POI are identified.\n",
    "\n",
    "### Validation Strategy\n",
    "Validation is a way to substantiate your machine learning algorithm's performance, i.e., to test how well your model has been trained. A classic validation mistake is testing your algorithm on the same data that is was trained on. Without separating the training set from the testing set, it is difficult to determine how well your algorithm generalises to new data. Another way of model validation is to perform a cross validation. In cross validation the data is split on k beans of equal size; runs learning experiments; repeats this operation number of times and take the average test result.\n",
    "\n",
    "### Algorithm Performance\n",
    "\n",
    "The two notable evaluation metrics for this POI identifier are precision and recall. The average precision for my decision tree classifier was 0.93673 and the average recall was 0.658. \n",
    "\n",
    "> *What do each of these mean?*\n",
    "\n",
    "*Precision* is how often our class prediction (POI vs. non-POI) is right when we guess that class. *Recall* is how often we guess the class (POI vs. non-POI) when the class actually occurred. In the context of our POI identifier, it is arguably more important to make sure we don't miss any POIs, so we don't care so much about precision. Imagine we are law enforcement using this algorithm as a screener to determine who to prosecute. When we guess POI, it is not the end of the world if they aren't a POI because we'll do due diligence. We won't put them in jail right away. For our case, we want high recall: when it is a POI, we need to make sure we catch them in our net. The decision tree algorithm performed best in recall of the algorithms I tried, hence it being my choice for final analysis.\n",
    "\n",
    "\n",
    "Stratification is a way of controlling for cofounders. Confounding is a major concern in causal studies because it results in biased estimation of exposure effects. In the extreme, this can mean that a causal effect is suggested where none exists, or that a true effect is hidden.Using, stratification we form a strata within which the cofuounding variables are approximately constant, estimating stratum-specific effects of exposure, checking to see wether these stratum-specific effects are roughly same, finally combing them to form a single estimate of thier common value.\n",
    "\n",
    "In this project, I used Stratified Shuffle Split with 1000 runs, which makes sure that the training and testing sets have about the equal ratio of passed vs. failed students. It also shuffles to remove the bias that ordering of students might have in the dataset. Second of all, overfitting might be a strong possibility. The number of training instances needed to accurately classify or predict a target label correctly increases exponentially as we increase the number of features. This is also known as the Curse of Dimensionality. See https://en.wikipedia.org/wiki/Curse_of_dimensionality We only have a relatively small number of training instances to consider. More data is generally better than less data, and we don’t have that advantage here.\n",
    "\n",
    "* criterion = 'entropy' \n",
    "* min_samples_split = 19\n",
    "* random_state = 75\n",
    "* min_samples_leaf=6 \n",
    "* max_depth = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Decision Tree Classifier</th>\n",
       "      <td>0.93673</td>\n",
       "      <td>0.83238</td>\n",
       "      <td>0.658</td>\n",
       "      <td>0.73499</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          Accuracy  Precision  Recall       F1\n",
       "Decision Tree Classifier   0.93673    0.83238   0.658  0.73499"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame([[0.93673, 0.83238, 0.65800, 0.73499]],\n",
    "             columns = ['Accuracy','Precision', 'Recall', 'F1'], \n",
    "             index = ['Decision Tree Classifier'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reflection \n",
    "\n",
    "Before the start of this project I was completely sure that building the machine learning is about choosing the right algorithm from the black box and some magic. Working on the person of interest identifier I've been recursively going through the process of data exploration, outlier detection and algorithm tuning and spend most of the time on a data preparation. The model performance raised significantly after missing values imputation, extra feature creation and feature selection and less after algorithm tuning which shows me once again how important to fit the model with the good data.\n",
    "\n",
    "In the context of our POI identifier, it is arguably more important to make sure we don't miss any POIs, so we don't care so much about precision. Imagine we are law enforcement using this algorithm as a screener to determine who to prosecute. When we guess POI, it is not the end of the world if they aren't a POI because we'll do due diligence. We won't put them in jail right away. For our case, we want high recall: when it is a POI, we need to make sure we catch them in our net. The decision tree algorithm performed best in recall (0.65) of the algorithms I tried, hence it being my choice for final analysis.\n",
    "\n",
    "This experience might be applied to other fraud detection tasks. I think there is way of the model improvement by using and tuning alternative algorithms like Random Forest.\n",
    "\n",
    "### Limitations of the study\n",
    "\n",
    "It’s important to identify and acknowledge the limitation of the study. My conclusions are based just on the provided data set which represent just 145 persons. To get the real causation, I should gather all financial and email information about all enron persons which is most probably not possible. Missing email values were imputed with median so the modes of the distributions of email features are switched to the medians. Algorithms were tuned sequentially (I've changed one parameter to achieve better performance and then swithched to another parameter. There is a chance that othere parameters in combination might give better model's accuracy)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
